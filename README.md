# Building-ETL-Pipeline-using-Dataflow-in-GCP
This project showcases a cloud-native data warehousing solution by integrating Google Cloud Storage (GCS) with BigQuery using Cloud Functions, enabling efficient data management and analysis. To begin, access permissions are configured by creating a service account with the necessary roles, including Storage Object Viewer for GCS access and BigQuery Data Editor for BigQuery access. Next, a Cloud Function trigger is set up to automate data ingestion, specifying the GCS bucket and event type (Finalize/Create) to execute the function when new data is uploaded. The Cloud Function itself is written in Python or Node.js, extracting data from GCS, transforming it using libraries like Pandas and NumPy, and loading it into BigQuery using the BigQuery client library. The function creates a BigQuery dataset and table, loads the transformed data, and handles errors and exceptions using try-except blocks. Logging and monitoring are also implemented using Stackdriver Logging and Monitoring. By integrating these GCP services, the project enables seamless data extraction, transformation, and loading, facilitating efficient data analysis and management.
